{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from data import iris\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, num_classes):\n",
    "        \n",
    "        super(IrisNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden2_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IrisNet(\n",
      "  (fc1): Linear(in_features=4, out_features=6, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=6, out_features=50, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=50, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = IrisNet(4, 6, 50, 3)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "num_epochs = 100\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 60\n",
    "iris_data_file = \"data/iris.data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'iris' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b5461c25546e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miris\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miris_data_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'iris' is not defined"
     ]
    }
   ],
   "source": [
    "train_ds, test_ds = iris.get_datasets(iris_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_ds, batch_size = batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(dataset=test_ds, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:  1.128388226032257\n",
      "Training loss:  1.0949336290359497\n",
      "Training loss:  1.0731183290481567\n",
      "Training loss:  1.0571289658546448\n",
      "Training loss:  1.0431751012802124\n",
      "Training loss:  1.0328028202056885\n",
      "Training loss:  1.0205643773078918\n",
      "Training loss:  1.0103479027748108\n",
      "Training loss:  1.0020192563533783\n",
      "Training loss:  0.9913573265075684\n",
      "Training loss:  0.9818470478057861\n",
      "Training loss:  0.9700731039047241\n",
      "Training loss:  0.9602098166942596\n",
      "Training loss:  0.9540480375289917\n",
      "Training loss:  0.9412203729152679\n",
      "Training loss:  0.9309024512767792\n",
      "Training loss:  0.925045520067215\n",
      "Training loss:  0.9112565219402313\n",
      "Training loss:  0.9030390977859497\n",
      "Training loss:  0.893351137638092\n",
      "Training loss:  0.8838623464107513\n",
      "Training loss:  0.8753636479377747\n",
      "Training loss:  0.8641993701457977\n",
      "Training loss:  0.8548792004585266\n",
      "Training loss:  0.8454445004463196\n",
      "Training loss:  0.8359574973583221\n",
      "Training loss:  0.8274637162685394\n",
      "Training loss:  0.8178788721561432\n",
      "Training loss:  0.8106752634048462\n",
      "Training loss:  0.8000161349773407\n",
      "Training loss:  0.7929991483688354\n",
      "Training loss:  0.781906247138977\n",
      "Training loss:  0.7742432355880737\n",
      "Training loss:  0.7645877003669739\n",
      "Training loss:  0.7579810321331024\n",
      "Training loss:  0.7495421469211578\n",
      "Training loss:  0.7393526434898376\n",
      "Training loss:  0.7353177666664124\n",
      "Training loss:  0.727630615234375\n",
      "Training loss:  0.7148464024066925\n",
      "Training loss:  0.7071609795093536\n",
      "Training loss:  0.6992088258266449\n",
      "Training loss:  0.6933554112911224\n",
      "Training loss:  0.6841509640216827\n",
      "Training loss:  0.6772503852844238\n",
      "Training loss:  0.6703224182128906\n",
      "Training loss:  0.6644665896892548\n",
      "Training loss:  0.656816303730011\n",
      "Training loss:  0.6498130857944489\n",
      "Training loss:  0.6432763934135437\n",
      "Training loss:  0.6382286846637726\n",
      "Training loss:  0.6307522356510162\n",
      "Training loss:  0.6249200403690338\n",
      "Training loss:  0.6213483214378357\n",
      "Training loss:  0.6132226288318634\n",
      "Training loss:  0.6094677150249481\n",
      "Training loss:  0.6030767858028412\n",
      "Training loss:  0.5970085561275482\n",
      "Training loss:  0.5935364365577698\n",
      "Training loss:  0.5870001316070557\n",
      "Training loss:  0.5820971429347992\n",
      "Training loss:  0.5772720575332642\n",
      "Training loss:  0.5729036629199982\n",
      "Training loss:  0.57277512550354\n",
      "Training loss:  0.565608024597168\n",
      "Training loss:  0.5601633787155151\n",
      "Training loss:  0.5558426678180695\n",
      "Training loss:  0.5517703592777252\n",
      "Training loss:  0.5477541387081146\n",
      "Training loss:  0.543931782245636\n",
      "Training loss:  0.5431593358516693\n",
      "Training loss:  0.5361093282699585\n",
      "Training loss:  0.5350855737924576\n",
      "Training loss:  0.5291953980922699\n",
      "Training loss:  0.5251411497592926\n",
      "Training loss:  0.523777037858963\n",
      "Training loss:  0.5183736383914948\n",
      "Training loss:  0.51555335521698\n",
      "Training loss:  0.5142800509929657\n",
      "Training loss:  0.5153615176677704\n",
      "Training loss:  0.5060331970453262\n",
      "Training loss:  0.5044602155685425\n",
      "Training loss:  0.5008413642644882\n",
      "Training loss:  0.49716418981552124\n",
      "Training loss:  0.49885550141334534\n",
      "Training loss:  0.49334411323070526\n",
      "Training loss:  0.4889645278453827\n",
      "Training loss:  0.48576895892620087\n",
      "Training loss:  0.4874062091112137\n",
      "Training loss:  0.48323020339012146\n",
      "Training loss:  0.47821152210235596\n",
      "Training loss:  0.47565819323062897\n",
      "Training loss:  0.47327397763729095\n",
      "Training loss:  0.4713049679994583\n",
      "Training loss:  0.4693453162908554\n",
      "Training loss:  0.4664802551269531\n",
      "Training loss:  0.46346063911914825\n",
      "Training loss:  0.4613148719072342\n",
      "Training loss:  0.4600942134857178\n",
      "Training loss:  0.46515895426273346\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    cum_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(inputs)\n",
    "#         output = output.long()\n",
    "#         print(output)\n",
    "        loss = criterion(output, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        cum_loss += loss.item()\n",
    "        \n",
    "    print(\"Training loss: \", cum_loss / len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.8836,  2.1516,  2.6020])\n",
      "tensor([ 2.6093,  0.0337, -1.6023])\n",
      "tensor([-2.4972,  2.2454,  3.0560])\n",
      "tensor([-1.2531,  1.3404,  1.5295])\n",
      "tensor([-1.1368,  1.6002,  1.6948])\n",
      "tensor([ 2.1571,  0.1774, -1.2677])\n",
      "tensor([-1.1865,  1.5431,  1.6979])\n",
      "tensor([-0.2647,  1.1824,  0.8227])\n",
      "tensor([-0.6678,  1.2808,  1.1456])\n",
      "tensor([-1.4698,  1.6689,  1.9667])\n",
      "tensor([ 2.4328,  0.0630, -1.4845])\n",
      "tensor([-0.7935,  1.3429,  1.2494])\n",
      "tensor([-2.1478,  2.1006,  2.7160])\n",
      "tensor([ 2.2773,  0.2266, -1.2844])\n",
      "tensor([ 2.9941, -0.0069, -1.8319])\n",
      "tensor([-0.3516,  1.0730,  0.7760])\n",
      "tensor([ 2.3606,  0.1085, -1.4168])\n",
      "tensor([-0.7360,  1.4369,  1.3156])\n",
      "tensor([ 2.3664,  0.0494, -1.4647])\n",
      "tensor([-1.1455,  1.4495,  1.5592])\n",
      "tensor([-1.3614,  1.5775,  1.7954])\n",
      "tensor([ 2.5535,  0.0595, -1.5495])\n",
      "tensor([-0.7556,  1.3286,  1.2366])\n",
      "tensor([-2.0049,  1.8848,  2.4456])\n",
      "tensor([-0.8601,  1.2374,  1.2299])\n",
      "tensor([ 2.4225,  0.0968, -1.4558])\n",
      "tensor([-1.0574,  1.4861,  1.5439])\n",
      "tensor([ 2.6517,  0.0667, -1.5931])\n",
      "tensor([-1.7125,  1.8539,  2.2577])\n",
      "tensor([-2.0150,  1.8127,  2.4086])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for data, label in test_loader:\n",
    "        counter = 0\n",
    "        for d in data:\n",
    "            print(model(d))\n",
    "#         if out[label] == out.max():\n",
    "#             print(\"Correct\")\n",
    "# #         print(maxPrediction)\n",
    "#         maxIndicies = [i for i in range(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

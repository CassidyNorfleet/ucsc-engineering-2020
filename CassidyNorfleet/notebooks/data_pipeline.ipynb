{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "41kgcjGVlizR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tarfile, gzip\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os as os\n",
        "import json, glob\n",
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "from itertools import chain\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "pd.set_option('display.max_rows', 30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ptlygYEgupt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if (os.path.exists(\"synthea-data-chf.tar.gz\") == False):\n",
        "    !wget http://public.gi.ucsc.edu/~rcurrie/synthea-data-chf.tar.gz\n",
        "if (os.path.exists(\"synthea-data-myocardial-infarction.tar.gz\") == False):\n",
        "    !wget http://public.gi.ucsc.edu/~rcurrie/synthea-data-myocardial-infarction.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4N1DRrQHauFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CHFDataset:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.dataset = None\n",
        "        self.trainingset = None\n",
        "\n",
        "    def set_dataset(self, path, sample_size=None):\n",
        "        self.dataset = {}\n",
        "\n",
        "        for json_patient in ExtractJsonFile(path, sample_size):\n",
        "            id = json_patient['entry'][0]['resource']['id']\n",
        "            self.dataset[id] = {}\n",
        "            self.dataset[id]['codes'] = []\n",
        "            self.dataset[id]['chf_first_discharge'] = None\n",
        "            self.dataset[id]['chf_rehosp'] = None\n",
        "\n",
        "            for bundle in ExtractCodes(json_patient):\n",
        "                if bundle['resource_type'] == \"Encounter\":\n",
        "                    if (bundle['encounter_code'] == \"IMP\" or bundle['encounter_code'] == \"EMER\") \\\n",
        "                    and bundle['reason_code'] == \"Chronic congestive heart failure (disorder)\":  \n",
        "                        if self.dataset[id]['chf_first_discharge'] is None:\n",
        "                            self.dataset[id]['chf_first_discharge'] = bundle['end_date']\n",
        "                        elif self.dataset[id]['chf_rehosp'] is None and (pd.date_range(self.dataset[id]['chf_first_discharge'], bundle['start_date']).shape[0] > 29):\n",
        "                            self.dataset[id]['chf_rehosp'] = bundle['start_date']\n",
        "                            break\n",
        "\n",
        "                self.dataset[id]['codes'].append([bundle['start_date'], bundle['reason_code']])\n",
        "\n",
        "    def set_trainingset(self, label=None, filter=None, window_range=24, step_size=0, num_datasets=1):\n",
        "        self.trainingset = DFUtil.generate_trainingset(self.dataset, label, filter, window_range, step_size, num_datasets)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wwdpYB57fJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MYINFDataset:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.dataset = None\n",
        "        self.trainingset = None\n",
        "\n",
        "    def set_dataset(self, path, sample_size=None):\n",
        "        self.dataset = {}\n",
        "\n",
        "        for json_patient in ExtractJsonFile(path, sample_size):\n",
        "            id = json_patient['entry'][0]['resource']['id']\n",
        "            self.dataset[id] = {}\n",
        "            self.dataset[id]['codes'] = []\n",
        "            self.dataset[id]['myinf_hosp'] = None\n",
        "\n",
        "            for bundle in ExtractCodes(json_patient):\n",
        "                if bundle['resource_type'] == \"Encounter\":\n",
        "                    if (bundle['encounter_code'] == \"IMP\" or bundle['encounter_code'] == \"EMER\") \\\n",
        "                    and bundle['reason_code'] == \"Myocardial Infarction\":  \n",
        "                        if self.dataset[id]['myinf_hosp'] is None:\n",
        "                            self.dataset[id]['myinf_hosp'] = bundle['start_date']\n",
        "                            break\n",
        "\n",
        "                self.dataset[id]['codes'].append([bundle['start_date'], bundle['reason_code']])\n",
        "\n",
        "    def set_trainingset(self, label=None, filter=None, window_range=24, step_size=0, num_datasets=1):\n",
        "        self.trainingset = DFUtil.generate_trainingset(self.dataset, label, filter, window_range, step_size, num_datasets)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE9wOcoDHzTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ExtractJsonFile:\n",
        "\n",
        "    def __init__(self, path, sample_size=None):\n",
        "        self.path = path\n",
        "        self.sample_size = sample_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        fhircodes = {}\n",
        "        counter, n = 0, 1\n",
        "\n",
        "        with tarfile.open(self.path, \"r:gz\") as tfile:\n",
        "            for member in tfile:\n",
        "                if (member.isdir()):\n",
        "                    continue\n",
        "\n",
        "                yield pd.read_json(tfile.extractfile(member))\n",
        "\n",
        "                counter = counter+1\n",
        "                if (counter == n):\n",
        "                    print(\"Processed \" + str(counter) + \" files\")\n",
        "                    n = n*2\n",
        "                if (self.sample_size == counter):\n",
        "                    break\n",
        "                    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ0PpqvXJIRg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ExtractCodes:\n",
        "\n",
        "    def __init__(self, json_patient):\n",
        "        self.json_patient = json_patient\n",
        "\n",
        "    def __iter__(self):\n",
        "        for entry in self.json_patient['entry']:\n",
        "            resource_type = entry['resource']['resourceType']\n",
        "            \n",
        "            if resource_type == \"Encounter\":\n",
        "                start_date = entry['resource']['period']['start'][0:10]\n",
        "                end_date = entry['resource']['period']['end'][0:10]\n",
        "                encounter_code = entry['resource']['class']['code']\n",
        "                try:\n",
        "                    reason_code = entry['resource']['reasonCode'][0]['coding'][0]['display']\n",
        "                except:\n",
        "                    reason_code = entry['resource']['type'][0]['coding'][0]['display']\n",
        "\n",
        "                yield {'resource_type':\"Encounter\", 'start_date':start_date, 'end_date':end_date, 'reason_code':reason_code, 'encounter_code':encounter_code}\n",
        "            \n",
        "            elif resource_type == \"Observation\":\n",
        "                start_date = entry['resource']['effectiveDateTime'][0:10]\n",
        "                reason_code = entry['resource']['code']['coding'][0]['display']\n",
        "                yield {'resource_type':\"Observation\", 'start_date':start_date, 'reason_code':reason_code}\n",
        "            \n",
        "            elif resource_type == \"Procedure\":\n",
        "                start_date = entry['resource']['performedPeriod']['start'][0:10]\n",
        "                reason_code = entry['resource']['code']['coding'][0]['display']\n",
        "                yield {'resource_type':\"Procedure\", 'start_date':start_date, 'reason_code':reason_code}\n",
        "            \n",
        "            elif resource_type == \"Condition\":\n",
        "                start_date = entry['resource']['onsetDateTime'][0:10]\n",
        "                reason_code = entry['resource']['code']['coding'][0]['display']\n",
        "                yield {'resource_type':\"Condition\", 'start_date':start_date, 'reason_code':reason_code}\n",
        "            \n",
        "            elif resource_type == \"Immunization\":\n",
        "                start_date = entry['resource']['occurrenceDateTime'][0:10]\n",
        "                reason_code = entry['resource']['vaccineCode']['coding'][0]['display']\n",
        "                yield {'resource_type':\"Immunization\", 'start_date':start_date, 'reason_code':reason_code}\n",
        "            \n",
        "            elif resource_type == \"MedicationRequest\":\n",
        "                start_date = entry['resource']['authoredOn'][0:10]\n",
        "                reason_code = entry['resource']['medicationCodeableConcept']['coding'][0]['display']\n",
        "                yield {'resource_type':\"MedicationRequest\", 'start_date':start_date, 'reason_code':reason_code}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQckeO7WHnC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DFUtil:\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_trainingset(dataset, label=None, filter=None, window_range=24, step_size=0, num_datasets=1):\n",
        "        df = pd.DataFrame.from_dict(dataset).T.reset_index().rename(columns={'index':'id'})\n",
        "        if label is not None: df = df.dropna(subset=[filter])\n",
        "\n",
        "        dfs = []\n",
        "        for i in range(num_datasets):\n",
        "            dfs.append(DFUtil.df_to_buckets(df, dataset, label, window_range, i*step_size))\n",
        "        \n",
        "        df = list(chain.from_iterable(dfs)) # Flatten multidimensional dfs in a single list\n",
        "        trainingset = DFUtil.normalize(df, label)\n",
        "        return trainingset\n",
        "\n",
        "    # Converts a DataFrame of codes into monthly buckets\n",
        "    @staticmethod\n",
        "    def df_to_buckets(df, dict_patients, label, window_range=24, offset=0):\n",
        "        frames = []\n",
        "\n",
        "        for index, row in df.iterrows():\n",
        "            id = row['id']\n",
        "\n",
        "            # Get specified range\n",
        "            try: # via label\n",
        "                end_range = pd.to_datetime(datetime.strptime(dict_patients[id][label], '%Y-%m-%d').date())\n",
        "            except: # via latest date\n",
        "                end_range = pd.to_datetime(datetime.strptime(row['codes'][-1][0], '%Y-%m-%d').date())\n",
        "            end_range = end_range + relativedelta(months=1) - relativedelta(months=offset)\n",
        "            start_range = end_range - relativedelta(months=window_range)\n",
        "            \n",
        "            # Set DataFrame to range\n",
        "            df = pd.DataFrame(row['codes']).rename(columns={0:'date', 1:'codes'})\n",
        "            try:\n",
        "                df['date'] = pd.to_datetime(df['date'])\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            df = df[df['date'].between(start_range, end_range)].set_index('date')\n",
        "\n",
        "            # Group codes by month\n",
        "            df = df.groupby(pd.Grouper(freq='M'))\n",
        "            df = df.aggregate(lambda x: tuple(x)).reset_index()\n",
        "\n",
        "            # Flatten df['codes'] into array[month][code]\n",
        "            arr_codes = []\n",
        "            for codes in df['codes']:\n",
        "                code_dict = {}\n",
        "                for code in codes:\n",
        "                    code_dict[code] = 1.0\n",
        "                arr_codes.append(code_dict)\n",
        "\n",
        "            # Add flattened codes to df\n",
        "            df = df.join(pd.DataFrame.from_dict(arr_codes).fillna(0)).drop(columns=['codes'])\n",
        "            # Fill in missing months\n",
        "            df.set_index('date', inplace=True)\n",
        "            df = df.reindex(pd.date_range(start_range, end_range, freq='M'), fill_value=0)\n",
        "            # Remove date as index\n",
        "            df = df.reset_index(drop=True)\n",
        "\n",
        "            # Add label\n",
        "            if (dict_patients[id][label] is not None and offset == 0):\n",
        "                df[label] = 1.0\n",
        "            else:\n",
        "                df[label] = 0.0\n",
        "\n",
        "            frames.append(df)\n",
        "            \n",
        "        return frames\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize(bucket_frames, training_label):\n",
        "        all_columns = []\n",
        "        for frame in bucket_frames:\n",
        "            all_columns.extend(x for x in frame.columns.tolist() if not x in all_columns)\n",
        "            \n",
        "        final_frames = []\n",
        "        for df in bucket_frames:\n",
        "            cols = df.columns.tolist()\n",
        "            cols.extend(x for x in all_columns if not x in cols)\n",
        "            df = df.reindex(columns=sorted(cols, reverse=False), fill_value=0)\n",
        "            col = df[training_label] # move training_label column to end of dateframe\n",
        "            df.drop(labels=[training_label], axis=1,inplace = True)\n",
        "            df[training_label] = col\n",
        "            final_frames.append(df)\n",
        "\n",
        "        return final_frames\n",
        "\n",
        "    @staticmethod\n",
        "    def shuffleColumns(dfs, training_label, num_shuffled):\n",
        "        dfs_shuffled = []\n",
        "        for _ in range(num_shuffled):\n",
        "            df_shuffled = dfs.copy()\n",
        "            # makes column labels the first row. (numpy only works with numbered columns so this preserves our label names)\n",
        "            df_shuffled[0] = pd.DataFrame(np.vstack([df_shuffled[0].columns, df_shuffled[0]]))\n",
        "            # randomize columns using numpy\n",
        "            arr = df_shuffled[0].to_numpy()\n",
        "            np.random.shuffle(arr.T)\n",
        "            # convert back to pandas dataframe\n",
        "            df_shuffled[0] = pd.DataFrame(arr)\n",
        "            df_shuffled[0].columns = df_shuffled[0].iloc[0]\n",
        "            df_shuffled[0] = df_shuffled[0].drop(df_shuffled[0].index[0]).reset_index(drop=True)\n",
        "            # move training_label to end of dataframe\n",
        "            col = df_shuffled[0][training_label]\n",
        "            df_shuffled[0].drop(labels=[training_label], axis=1, inplace = True)\n",
        "            df_shuffled[0][training_label] = col\n",
        "            # reindex all dfs on df_shuffled[0]\n",
        "            for i in range(len(df_shuffled)):\n",
        "              df_shuffled[i] = df_shuffled[i].reindex(df_shuffled[0].columns, axis=1)\n",
        "\n",
        "            dfs_shuffled.append(df_shuffled)\n",
        "\n",
        "        return dfs_shuffled"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPSViJtiH-jj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OSUtil:\n",
        "\n",
        "    @staticmethod\n",
        "    def trainingset_from_csv(path):\n",
        "        dfs_trainingset = []\n",
        "        files = os.listdir(path)\n",
        "        for file in files:\n",
        "          try:\n",
        "            dfs_trainingset.append(pd.read_csv(file, index_col ='Unnamed: 0'))\n",
        "          except Exception as e:\n",
        "            print(e)\n",
        "            continue\n",
        "        return dfs_trainingset\n",
        "\n",
        "    @staticmethod\n",
        "    def export_csv(path, dfs, overwrite=False):\n",
        "        if (os.path.exists(path) == False):\n",
        "            !mkdir $path\n",
        "        if (overwrite == True):\n",
        "            !rm $path\"/*.csv\"\n",
        "        print('exporting ' + path)\n",
        "        for i in range(len(dfs)):\n",
        "            dfs[i].to_csv(path + '/patient' + str(i) + '.csv')\n",
        "        print('done')\n",
        "\n",
        "    @staticmethod\n",
        "    def zip_folder(input_path, output_path):\n",
        "        print('compressing ' + input_path)\n",
        "        !zip -r $output_path\".zip\" $input_path\n",
        "        print('finished')\n",
        "\n",
        "    @staticmethod\n",
        "    def delete_folder(path):\n",
        "        !rm -rf $path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1oIlMGCT2WS",
        "colab_type": "text"
      },
      "source": [
        "### Create training data from congestive heart failure patients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5poU3j5gaoX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chf = CHFDataset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap2wEPtUI6zE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chf.set_dataset(\"synthea-data-chf.tar.gz\", sample_size=16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fxoJhoaIwwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chf.set_trainingset(label=\"chf_rehosp\", filter=\"chf_first_discharge\", window_range=24)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUVvc1IqUBts",
        "colab_type": "text"
      },
      "source": [
        "### Create training data from myocardial infarction patients\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E37cVuG37UrF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "myinf = MYINFDataset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQhgeJDYI8Fk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "myinf.set_dataset(\"synthea-data-myocardial-infarction.tar.gz\", sample_size=16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZqOZjzzIvoX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "myinf.set_trainingset(label=\"myinf_hosp\", filter=\"myinf_hosp\", window_range=6, step_size=1, num_datasets=6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntol8uWbps1x",
        "colab_type": "text"
      },
      "source": [
        "### Export dataframes to csv files and zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVVfRK9yvi_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#OSUtil.export_csv(\"csv-chf\", chf.trainingset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS0k0iZvMXCX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#OSUtil.zip_folder('csv-chf', 'csv-chf')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}